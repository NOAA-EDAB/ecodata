---
title: 'State of the Ecosystem: Indicator Processing & Visualization'
author: Ecosystems Dynamics and Assessment Branch
output:
  rmarkdown::html_document:
    toc: true
    toc_float: true
    theme: lumen
---

## Introduction
The purpose of this report is to document State of the Ecosystem (SOE) indicator data processing. All R code used to process and visualize the following data sets is self-contained in the [Rmarkdown document](https://github.com/NOAA-EDAB/soe/blob/master/inst/Rmd/process_raw.Rmd) associated with this HTML file. To run and update data sets in this document, set the `save_clean` parameter in the set-up chunk to `TRUE`. Raw data for these indicators are available in the file directory `soe/inst/extdata`.

```{r, message=FALSE, echo = F}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align='center')

#Required libraries
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(here)
library(zoo)
library(kableExtra)
library(sf)
library(rgdal)
library(raster)
library(sp)
library(gridExtra)
library(htmlwidgets)
library(corrplot)
library(tseries)
library(heatmaply)


#Data directories
raw.dir <- here("inst","extdata") #raw data directory
clean.dir <- here("data") #output directory for cleaned data
gis.dir <- here("inst","extdata","gis")
sample.dir <- here("inst","extdata","sample")

#CRS
crs <- "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"

#Write output to file
save_clean <- F

#Execute spatial processing (must be TRUE to write clean data to file). If FALSE, will load sample data from file for plotting example
spatial_processing <- F



```


## Data sets

### Surface winds {.tabset .tabset-fade}

These data are sourced from the [NCEP North American Regional Reanalysis (NARR)](https://www.esrl.noaa.gov/psd/data/gridded/data.narr.monolevel.html), extending from January 1979 to September 2018. 

```{r surface_winds_vars, echo = FALSE}

fname <- "NCEP NARR surface wind; TKE; HLCY, monthly, 1979-2018, V1.csv"

vars <- data.frame(Variable = c("Wind speed",
                                "Wind direction",
                                "Turbulent kinetic energy",
                                "Storm relative helicity"),
                   Name = c("uwnd",
                            "vwnd",
                            "tke",
                            "hlcy"),
                   Units = c("m sec^-1^",
                             "°",
                             "J kg^-1^",
                             "m^2^sec^-2^"))

kable(vars, caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(3, width = "5cm")
```


Variables included in these data are surface wind speed and direction (*uwnd* and *vwnd* respectively), surface turbulent kinetic energy (TKE), and storm relative helicity (HLCY). An indicator for total wind speed is calculated below as
$$\textrm{TWS} = \sqrt{u^2 + v^2}$$. Data are visualized seasonally (Fall = October, November, December; Winter = January, February, March; Spring = April, May, June; Summer = July, August, September).

**Filename**: NCEP NARR surface wind; TKE; HLCY, monthly, 1979-2018, V1.csv  
**Contributor**: Vincent Saba, (vincent.saba@noaa.gov)

#### Processing

```{r surface_winds}
# Read in raw data
d <- read.csv(file.path(raw.dir,"NCEP NARR surface wind; TKE; HLCY, monthly, 1979-2018, V1.csv"))

# Processing steps for all wind speed data
wind_clean1 <- d  %>% gather(., Var, Value, GB.uwnd:MAB.tke) %>% #convert wide to long
  dplyr::rename(Time = Month.Year) %>% #rename time variable
  separate(Var, c("EPU","Var"),"\\.") %>% #separate out EPU from variable names
    mutate(Time = dmy(.$Time), #Convert to date format
         Units = plyr::mapvalues(Var, from = unique(Var), to = c(rep("J/kg",2),"m^2/sec^2","J/kg")), #add units
         Time, season = plyr::mapvalues(month(Time), from = seq(1,12,1), #Get season
                                         to = c(rep("winter",3),
                                                rep("spring",3),
                                                rep("summer",3),
                                                rep("fall",3)))) 


# Calculate total wind speed from u and v components
total_wind_speed <- wind_clean1 %>% 
  filter(Var == "uwnd"| Var == "vwnd") %>% #select variables
  spread(., Var, Value) %>% #convert to wide for calculating tws
  mutate(`total wind speed` = sqrt(uwnd^2 + vwnd^2)) %>%  #tws
  dplyr::select(-uwnd, -vwnd) %>% #start processing back to SOE format
  gather(.,Var, Value, `total wind speed`) #convert to long

wind_clean <- rbind(wind_clean1, total_wind_speed)
wind_clean <- wind_clean %>%
  unite(., Var, c(Var, season), sep = " ") %>% #merge season into Var column
  group_by(Time = year(Time), EPU, Var, Units) %>% 
  dplyr::summarise(Value = mean(Value)) %>% 
  as.data.frame()

if (save_clean){
save(wind_clean, file =
       file.path(clean.dir, "wind_clean.Rdata"))
}

```

#### MAB Total Wind Speed

```{r wind_speed_vis, echo = FALSE}
mab <- wind_clean %>%
  filter(str_detect(Var, "total wind speed"), EPU == "MAB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab("Wind Speed (m/s)") +
  ggtitle("MAB Total Wind Speed") +
  theme_bw()+
  theme(strip.background = element_blank()) 

mab
```

#### GB Total Wind Speed

```{r GB tws, echo = FALSE}
gb <- wind_clean %>%
  filter(str_detect(Var, "total wind speed"), EPU == "GB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab("Wind Speed (m/s)") +
  ggtitle("GB Total Wind Speed") +
  theme_bw()+
  theme(strip.background = element_blank()) 

gb
```

#### GOM Total Wind Speed

```{r GOM tws, echo = FALSE}
gom <- wind_clean %>%
  filter(str_detect(Var, "total wind speed"), EPU == "GOM") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab("Wind Speed (m/s)") +
  ggtitle("GOM Total Wind Speed") +
  theme_bw()+
  theme(strip.background = element_blank()) 

gom
```

#### MAB Helicity

```{r MAB hel, echo = FALSE}
mab <- wind_clean %>%
  filter(str_detect(Var, "hcly"), EPU == "MAB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Relative Helicity (m"^2*" sec"^-2*")")) +
  ggtitle("MAB Storm Relative Helicity ") +
  theme_bw()+
  theme(strip.background = element_blank()) 

mab
```

#### GB Helicity

```{r GB hel, echo = FALSE}
gb <- wind_clean %>%
  filter(str_detect(Var, "hcly"), EPU == "GB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Relative Helicity (m"^2*" sec"^-2*")")) +
  ggtitle("GB Storm Relative Helicity ") +
  theme_bw()+
  theme(strip.background = element_blank()) 

gb
```

#### GOM Helicity

```{r GOM hel, echo = FALSE}
gom <- wind_clean %>%
  filter(str_detect(Var, "hcly"), EPU == "GOM") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Relative Helicity (m"^2*" sec"^-2*")")) +
  ggtitle("GOM Storm Relative Helicity ") +
  theme_bw()+
  theme(strip.background = element_blank()) 

gom
```

#### MAB TKE

```{r MAB tke, echo = FALSE}
mab <- wind_clean %>%
  filter(str_detect(Var, "tke"), EPU == "MAB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Total Kinetic Energy (J kg"^-1*")")) +
  ggtitle("MAB Total Kinetic Energy ") +
  theme_bw()+
  theme(strip.background = element_blank()) 

mab
```

#### GB TKE

```{r GB tke, echo = FALSE}
gb <- wind_clean %>%
  filter(str_detect(Var, "tke"), EPU == "GB") %>% #filter
  mutate(Var = word(Var, -1)) %>% 
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Total Kinetic Energy (J kg"^-1*")")) +
  ggtitle("GB Total Kinetic Energy ") +
  theme_bw()+
  theme(strip.background = element_blank()) 

gb
```


#### GOM TKE

```{r GOM tke, echo = FALSE}
gom <- wind_clean %>%
  filter(str_detect(Var, "tke"), EPU == "GOM") %>% #filter
  mutate(Var = word(Var, -1)) %>%
  ggplot()+ #plot
  geom_line(aes(x = Time, y = Value))+
  facet_wrap(.~Var, nrow = 2)+
  ylab(expression("Total Kinetic Energy (J kg"^-1*")")) +
  ggtitle("GOM Total Kinetic Energy ") +
  theme_bw()+
  theme(strip.background = element_blank())

gom
```

### Slopewater proportions {.tabset .tabset-fade}

Slopewater proportions give the percent total of water type observed in the deep Northeast Channel (150-200 m depth). 

```{r slopewater_prop_vars, echo = FALSE}

fname <- "slopewater_proportions.csv"

vars <- data.frame(Variable = c("Warm Slope Water proportion",
                                "Labrador Shelf Slope Water proportion"),
                   Names = c("WSW",
                             "LSLW"),
                   Units = c("unitless",
                             "unitless"))

kable(vars, caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(2, width = "5cm")
```

Raw data fields correspond to year, water mass flavor (WSW = Warm Slope Water, LSLW = Labrador Slope Water), and proportion of total expressed as a percentage. 

**Filename**: slopewater_proportions.csv  
**Contributor**: Paula Fratantoni (paula.fratantoni@noaa.gov)

#### Processing

```{r slopewater_proportions}
d <- read.csv(file.path(raw.dir,"slopewater_proportions.csv"))

slopewater <- d %>%
  dplyr::rename(Time = year, Var = water.mass.flavor, Value = prop) %>% 
  mutate(EPU = "GOM", Units = "unitless", Var2 = "proportion ne channel") %>% 
  unite(.,Var,c(Var,Var2), sep = " ")

if (save_clean){
save(slopewater, file =
       file.path(clean.dir, "slopewater_proportions.Rdata"))
}

```

#### Visualization

```{r slopewater_vis}
slopewater %>% 
  mutate(Var, Var = plyr::mapvalues(Var, from = c("WSW proportion ne channel",
                                                  "LSLW proportion ne channel"),
                                    to = c("WSW","LSLW"))) %>% 
  dplyr::rename(Flavor  = Var) %>% 
ggplot() +
  geom_line(aes(x = Time, y = Value, color = Flavor))+
  geom_point(aes(x = Time, y = Value, color = Flavor)) +
  ylab("Percent of Total Slopewater") +
  ggtitle("Slopewater Proportions in NE Channel")+
  theme_bw()+
  theme(strip.background = element_blank())

```


### Ocean temperature anomaly {.tabset .tabset-fade}

These data include *in situ* regional time series of both surface and bottom water temperature anomalies on the Northeast Continental Shelf. Raw data is split into four files by EPU (SS, GOM, GB, and MAB).

```{r ocean_temp_insitu_vars, echo = FALSE}

fname <- "Eco{EPU}_core_Ttopbot.csv"

vars <- data.frame(Variable = c("SST anomaly",
                                "Reference SST (1981-2010)",
                                "Bottom temp. anomaly",
                                "Reference BT (1981-2010)"),
                   Names = c("Tsfc_anom",
                             "Tsfc_ref",
                             "Tbot_anom",
                             "Tbot_ref"),
                   Units = c("°C",
                             "°C",
                             "°C",
                             "°C"))

kable(vars, caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(3, width = "5cm")
```

**Filenames**: EcoSS_core_Ttopbot.csv, EcoGoM_core_Ttopbot.csv, EcoGB_core_Ttopbot.csv, EcoMAB_core_Ttopbot.csv  
**Contributor**: Paula Fratantoni (paula.fratantoni@noaa.gov)

#### Processing

```{r ocean_temperature}
ss <- read.csv(file.path(raw.dir,"EcoSS_core_Ttopbot.csv")) %>% mutate(EPU = "SS")
gom <- read.csv(file.path(raw.dir,"EcoGoM_core_Ttopbot.csv")) %>% mutate(EPU = "GOM")
gb <- read.csv(file.path(raw.dir,"EcoGB_core_Ttopbot.csv")) %>% mutate(EPU = "GB")
mab <- read.csv(file.path(raw.dir,"EcoMAB_core_Ttopbot.csv")) %>% mutate(EPU = "MAB")

ocean_temp_insitu <- rbind(ss, gom, gb, mab) %>% #bind all
  dplyr::rename(Time = decimal.year, Var = variable.name, Value = temperature) %>% #rename
  mutate(Units = "degreesC", Time = as.Date(format(date_decimal(Time), "%Y-%b-%d"), "%Y-%b-%d"),
         Var, Var = plyr::mapvalues(Var, from = c("Tsfc_anom",#Rename variables
                             "Tsfc_ref",
                             "Tbot_anom",
                             "Tbot_ref"),
                             to = c("sst anomaly in situ",
                                "reference sst in situ (1981-2010)",
                                "bottom temp anomaly in situ",
                                "reference bt in situ (1981-2010)"))) %>% 
  group_by(Time = year(Time), EPU, Var, Units) %>%
  dplyr::summarise(Value = mean(Value)) %>%
  as.data.frame() 

if (save_clean){
save(ocean_temp_insitu, file =
       file.path(clean.dir, "ocean_temp_insitu.Rdata"))
}
```

#### Visualization
```{r ocean_temp_vis, fig.height=7,fig.width=10, echo = FALSE}

one <- ocean_temp_insitu %>%
  filter(Var == "sst anomaly in situ") %>% 
ggplot2::ggplot() +
  geom_line(aes(x = Time, y = Value)) +
  geom_point(aes(x = Time, y = Value), size = 1) +
  facet_grid(.~EPU) +
  ylab(expression("Temp. Anomaly ("*degree*"C)")) +
  ggtitle("SST") +
  theme_bw()+
  theme(strip.background = element_blank())

two <- ocean_temp_insitu %>%
 filter(Var == "bottom temp anomaly in situ") %>%
ggplot2::ggplot() +
  geom_line(aes(x = Time, y = Value)) +
  geom_point(aes(x = Time, y = Value), size = 1) +
  facet_grid(.~EPU) +
  ylab(expression("Temp. Anomaly ("*degree*"C)")) +
  ggtitle("Bottom temperature") +
  theme_bw()+
  theme(strip.background = element_blank())

grid.arrange(one, two)
```


### Ocean salinity anomaly {.tabset .tabset-fade}

These data include *in situ* regional time series of both surface and bottom salinity anomalies on the Northeast Continental Shelf. Raw data is split into four files by EPU (SS, GOM, GB, and MAB).

```{r ocean_salinity_insitu_vars, echo = FALSE}

fname <- "Eco{EPU}_core_Stopbot.csv"

vars <- data.frame(Variable = c("Surface salinity anomaly",
                                "Reference surface salinity (1981-2010)",
                                "Bottom salinity anomaly",
                                "Reference bottom salinity (1981-2010)"),
                   Names = c("Ssfc_anom",
                             "Ssfc_ref",
                             "Sbot_anom",
                             "Sbot_ref"),
                   Units = c("PSU",
                             "PSU",
                             "PSU",
                             "PSU"))

kable(vars, caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(2, width = "5cm")
```

**Filenames**: EcoSS_core_Stopbot.csv, EcoGoM_core_Stopbot.csv, EcoGB_core_Stopbot.csv, EcoMAB_core_Stopbot.csv  
**Contributor**: Paula Fratantoni (paula.fratantoni@noaa.gov)

#### Processing

```{r ocean_salinity}
ss <- read.csv(file.path(raw.dir,"EcoSS_core_Stopbot.csv")) %>% mutate(EPU = "SS")
gom <- read.csv(file.path(raw.dir,"EcoGoM_core_Stopbot.csv")) %>% mutate(EPU = "GOM")
gb <- read.csv(file.path(raw.dir,"EcoGB_core_Stopbot.csv")) %>% mutate(EPU = "GB")
mab <- read.csv(file.path(raw.dir,"EcoMAB_core_Stopbot.csv")) %>% mutate(EPU = "MAB")

ocean_sal_insitu <- rbind(ss, gom, gb, mab) %>% #bind all
  dplyr::rename(Time = decimal.year, Var = variable.name, Value = salinity) %>% #rename
  mutate(Units = "PSU", Time = as.Date(format(date_decimal(Time), "%Y-%b-%d"), "%Y-%b-%d"),
         Var, Var = plyr::mapvalues(Var, from = c("Ssfc_anom",
                             "Ssfc_ref",
                             "Sbot_anom",
                             "Sbot_ref"),
                     to = c("surface salinity anomaly in situ",
                        "reference surface salinity in situ (1981-2010)",
                        "bottom salinity anomaly in situ",
                        "reference bottom salinity in situ (1981-2010)"))) %>% 
  group_by(Time = year(Time), EPU, Var, Units) %>%
  dplyr::summarise(Value = mean(Value)) %>%
  as.data.frame()

if (save_clean){
save(ocean_sal_insitu, file =
       file.path(clean.dir, "ocean_sal_insitu.Rdata"))
}
```

#### Visualization

```{r salinity_vis, fig.width = 10, fig.height=7, warning  = FALSE, message = FALSE, echo = FALSE}

one <- ocean_sal_insitu %>%
  filter(Var == "surface salinity anomaly in situ") %>% 
ggplot2::ggplot() +
  geom_line(aes(x = Time, y = Value)) +
  geom_point(aes(x = Time, y = Value), size = 1) +
  facet_grid(.~EPU) +
  ylab("Salinity Anomaly (PSU)") +
  ggtitle("Surface salinity") +
  theme_bw()+
  theme(strip.background = element_blank())

two <- ocean_sal_insitu %>%
 filter(Var == "bottom salinity anomaly in situ") %>%
ggplot2::ggplot() +
  geom_line(aes(x = Time, y = Value)) +
  geom_point(aes(x = Time, y = Value), size = 1) +
  facet_grid(.~EPU) +
  ylab("Salinity Anomaly (PSU)") +
  ggtitle("Bottom salinity") +
  theme_bw()+
  theme(strip.background = element_blank())

grid.arrange(one, two)
```


### Stratification {.tabset .tabset-fade}

These data are time series of average stratification (0-50 m depth) by EPU. 

**Filename**: Strat50.csv
**Contributor**: Paula Fratantoni (paula.fratantoni@noaa.gov)

```{r stratification_vars, echo = FALSE}

fname <- "Strat50.csv"

vars <- data.frame(Variable = c("stratification"),
                   Names = c("stratification"),
                   Units = c("kg m ^-3^"))

kable(vars, caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(2, width = "5cm")
```

#### Processing

```{r stratification}

strat <- read.csv(file.path(raw.dir, "Strat50.csv"), stringsAsFactors = FALSE)

stratification <- strat %>% 
  dplyr::rename(Time = time, Var = var, Value = stratification) %>% 
  separate(., Var, c("Var","EPU"), sep = "_") %>% 
  mutate(Var = "stratification (0-50 m)",
         Units = "kg m^-3")

if (save_clean){
  save(stratification, file = file.path(clean.dir, "stratification.Rdata"))
}
```

#### Visualization

```{r strat_vis, message=F, warning=F, fig.width = 10, fig.height = 3.25, echo = F}

ggplot(data =stratification)+
  geom_line(aes(x = Time, y = Value)) +
  geom_point(aes(x = Time, y = Value)) +
  facet_grid(.~EPU) +
  theme_bw() +
  ylab(expression("Stratification (kg m"^-3*")")) +
  theme(strip.background = element_blank())

```


### EcoMon nutrient data {.tabset}

These data include nutrient concentrations, temperature, salinity, density, and dissolved oxygen data sampled via CTD profiles on Ecosystem Monitoring (EcoMon) cruises between 11/3/2009 - 10/19/2016. More metadata are available [here](https://www.nodc.noaa.gov/oads/data/0127524.xml).  
</br>
<div align="center">
```{r EcoMon_vars, echo = FALSE}

fname <- "EcoMon Nutrient Data Through June 2018.csv"

vars <- data.frame(Variable = c("Cruise identifier","Cruise identifier","Station number",
                                "CTD cast number","Sample bottle number","Sample date",
                                "Sample time","Latitude","Longitude","Depth of station",
                                "Depth of sample","Water pressure","Water temperature",
                                "Water salinity","Potential density at surface pressure",
                                "Dissolved oxygen","Silicic acid concentration",
                                "Total nitrate and nitrite concentration","Ammonia concentration",
                                "Phosphate concentration","Dissolved oxygen"),
                   Names = c("EXPOCODE","Cruise_ID","STNNBR","CASTNO",
                             "BTLNBR","Date_UTC","Time_UTC",
                             "Latitude","Longitude","Depth_station",
                             "Depth_sampling","CTDPRS","CTDTEMP",
                             "CTDSAL","Sigma.Theta","CTDOXY",
                             "SILCAT","NITRIT+NITRAT","AMMMONIA",
                             "PHSPHT","CTDOXYMOL"),
                   Units = c("","","",
                             "","","MM/DD/YYYY",
                             "hh:mm","decimal degrees","decimal degrees",
                             "m","m","decibars","°C",
                             "PSS-78","kg m^-3^","mg L^-1^",
                             "$\\mu$M","$\\mu$M","$\\mu$M",
                             "$\\mu$M","$\\mu$mol kg^-1^"))

kable(vars,caption = paste0('Variables in "',fname,'"'))%>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center") %>%
  column_spec(2, width = "5cm") %>% 
  scroll_box(height = "400px")
```
</div>  
</br>
**Filename**: EcoMon Nutrient Data Through June 2018.csv  
**Contributor**: Chris Melrose (chris.melrose@noaa.gov)  

#### Processing

```{r EcoMon nutrients, message=FALSE, warning=FALSE}
d <- read.csv(file.path(raw.dir,"EcoMon Nutrient Data Through June 2018.csv"), stringsAsFactors = FALSE)

#Create data frame for mapping units to variable names
mapping <- data.frame(Units = as.character(d[1,]),
                      Var = as.character(names(d)))
mapping[mapping$Units == "" | mapping$Units == "NA",]$Units <- NA

#remove row with units
d <- slice(d,-1)

d1 <- d %>% 
  mutate(Time = Date_UTC) %>% #create Time variable
  dplyr::select(-Date_UTC,-Time_UTC) %>% #remove time, date
  gather(., Var, Value, -Latitude, -Longitude, -Time, -Depth_sampling, -Depth_station) %>% #turn wide to long while retaining lat/lon
  filter(!is.na(Value)) %>% #remove NA
  left_join(., mapping, by = c("Var")) %>% #join units 
  mutate(Longitude = as.numeric(Longitude),
         Latitude = as.numeric(Latitude),
         Time = mdy(Time)) %>% 
  filter(Latitude > 32, Latitude<50)


#Sanity check
# t1 <- d1[d1$Var == "CTDOXYMOL" ,]$Value
# t <-  d %>% slice(.,-1)
# t <- as.character(t$CTDOXYMOL)
# all(t == t1)

#Read in EPU shapefile
epu <- readOGR(file.path(gis.dir, "EPU_Extended.shp"), verbose = F) 
epu <- as(epu, "sf") #convert to sf object

if(spatial_processing){

  #Test maps
  #All EPUs
  #ggplot() + geom_sf(data = epu)
  
  #Scotian shelf
  # ss <- epu %>% filter(EPU == "SS")
  # ggplot() + geom_sf(data = ss)
  
  #get latitude and longitude for creating SpatialPointsDataFrame
  lat <-  as.numeric(d$Latitude)
  lon <- as.numeric(d$Longitude)
  coords <- data.frame(lon = lon,lat = lat)
  
  #create spdf
  spdf <- SpatialPointsDataFrame(coords = coords, data = coords,
                                 proj4string = CRS(crs))
  #convert to sf
  coords_sf <- st_as_sf(spdf) 
  
  #get intersection for mapping EPUs back to nutrient data
  epu_intersect <- st_intersection(epu, coords_sf)
  #plot(epu_intersect[epu_intersect$EPU == "MAB",])
  
  #Map back to nutrient data frame
  epu_df <- data.frame(Longitude = epu_intersect$lon,
                       Latitude = epu_intersect$lat,
                       EPU = epu_intersect$EPU)
  #join
  NE_LME_nutrients <- d1 %>% 
    left_join(.,epu_df, by = c("Latitude","Longitude"))
  
  #Select data for plotting 
  Nitr <- NE_LME_nutrients %>% filter(Var == "NITRIT.NITRAT")
  
  #Back to SOE format and specify bottom, mid-water, or surface sampling
  NE_LME_nutrients <- NE_LME_nutrients %>%
    dplyr::select(-Latitude, -Longitude) %>% 
    mutate(Value = as.numeric(Value),
           Depth_station = as.numeric(Depth_station),
           Depth_sampling = as.numeric(Depth_sampling)) %>% 
    mutate(bot_dif = Depth_station-Depth_sampling) %>% 
    mutate(surf_bot = ifelse(bot_dif <= 10, "bottom",
                           ifelse(bot_dif > 10 & Depth_sampling <= 5, "surface", "mid-water"))) %>% 
    filter(Value > 0, !is.na(EPU), !Var %in% c("BTLNBR","CASTNO","Depth_sampling",
                                             "Depth_station","STNNBR")) %>% 
    mutate(Var = paste(Var, surf_bot)) %>% 
    dplyr::select(Time, Var, Value, Units, EPU) %>% 
    group_by(EPU, Time = year(Time), Var, Units) %>% 
    dplyr::summarise(Value = mean(Value, na.rm = TRUE)) %>% 
    as.data.frame()
  
  if (save_clean){
    save(NE_LME_nutrients,file = file.path(clean.dir, "EcoMon_nutrients.Rdata"))
  }
} else {
  load(file.path(sample.dir,"sample_nutrients.Rdata"))
  load(file.path(clean.dir,"EcoMon_nutrients.Rdata"))
}






```

#### QA

```{r ecomon_plotting1, echo = F}
#Confirm transformation
ggplot() + 
  geom_sf(data = epu) +
  geom_point(data = Nitr, aes(x = Longitude, y = Latitude, color = EPU)) +
  ggtitle("Mapping EcoMon Nutrient Data to EPU") +
  theme_bw() 
```


#### Surface Nutrients

```{r surface_nutrients, echo = F}

N <- NE_LME_nutrients %>% 
  filter(Var == "NITRIT.NITRAT surface") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Nitrit+Nitrat ("*mu*"mol kg"^-1*")"))+
  ggtitle("Nitrit+Nitrat")+
  theme_bw()+
  theme(strip.background = element_blank())+
  guides(colour=FALSE)

P <- NE_LME_nutrients %>% 
  filter(Var == "PHSPHT surface") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Phosphate ("*mu*"mol kg"^-1*")"))+
  ggtitle("Phosphate")+
  theme_bw()+
  theme(strip.background = element_blank())+
  guides(colour=FALSE)

ammon <- NE_LME_nutrients %>% 
  filter(Var == "AMMONIA surface") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Ammonia ("*mu*"mol kg"^-1*")"))+
  ggtitle("Ammonia")+
  theme_bw()+
  theme(strip.background = element_blank(),legend.position = c(0.85, 0.6),
        legend.key = element_rect(color="transparent"),
        legend.background = element_rect(fill="transparent"))

silcat <- NE_LME_nutrients %>% 
  filter(Var == "SILCAT surface") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Silicates ("*mu*"mol kg"^-1*")"))+
  ggtitle("Silicates")+
  theme_bw()+
  guides(color= FALSE)

cowplot::plot_grid(N, P,ammon ,silcat)

```

#### Bottom Nutrients

```{r bottom_nutrients, echo = F}

N <- NE_LME_nutrients %>% 
  filter(Var == "NITRIT.NITRAT bottom") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Nitrit+Nitrat ("*mu*"mol kg"^-1*")"))+
  ggtitle("Nitrit+Nitrat")+
  theme_bw()+
  theme(strip.background = element_blank())+
  guides(colour=FALSE)

P <- NE_LME_nutrients %>% 
  filter(Var == "PHSPHT bottom") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Phosphate ("*mu*"mol kg"^-1*")"))+
  ggtitle("Phosphate")+
  theme_bw()+
  theme(strip.background = element_blank())+
  guides(colour=FALSE)

ammon <- NE_LME_nutrients %>% 
  filter(Var == "AMMONIA bottom") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Ammonia ("*mu*"mol kg"^-1*")"))+
  ggtitle("Ammonia")+
  theme_bw()+
  theme(strip.background = element_blank(),legend.position = c(0.85, 0.6),
        legend.key = element_rect(color="transparent"),
        legend.background = element_rect(fill="transparent"))

silcat <- NE_LME_nutrients %>% 
  filter(Var == "SILCAT bottom") %>% 
  ggplot() +
  geom_line(aes(x = Time, y = Value, color = EPU)) +
  ylab(expression("Silicates ("*mu*"mol kg"^-1*")"))+
  ggtitle("Silicates")+
  theme_bw()+
  guides(color= FALSE)

cowplot::plot_grid(N, P,ammon ,silcat)

```

### North Atlantic Oscillation (NAO) {.tabset}

North Atlantic Oscillation data were taken from the [NOAA NWS Climate Prediction Center](http://www.cpc.ncep.noaa.gov/data/teledoc/nao.shtml). These data show the monthly NAO index time series beginning in 1950 and ending in October 2018. The index is standardized to the 1981-2010 reference period. More information regarding the methodology involved in deriving the NAO and its significance is available [here](http://www.cpc.ncep.noaa.gov/data/teledoc/nao.shtml).

#### Processing

```{r nao_index}
d <- read.csv(file.path(raw.dir, "NAO_index_1950-Oct2018.csv")) %>% slice(.,-826:-825)

#Three month running average
nao_cpc <- d %>% 
  mutate(Time = paste0(YEAR,":Q",rep(1:4, each = 3))) %>% 
  group_by(Time) %>% 
  dplyr::summarise(Value = mean(INDEX)) %>% 
  mutate(Var = "nao index", units = "unitless", EPU = "all",
         Time = yq(Time))

#annual average
nao_annual <- d %>% 
  group_by(YEAR) %>% 
  dplyr::summarise(Value = mean(INDEX),
                   Variance = var(INDEX)) %>% 
  mutate(Var = "nao index", Units = "unitless", EPU = "all") %>% 
  dplyr::rename(Time = YEAR)

if (save_clean){
  nao <- nao_annual %>% select(-Variance) 
  save(nao, file = file.path(clean.dir, "nao_annual.Rdata"))
}

```

#### NAO index and index variance (Annual)

```{r NAO_index, echo = FALSE, fig.height=8}
ind_annual <- nao_annual %>% 
  ggplot()+
  geom_line(aes(x = Time, y = Value)) +
  geom_hline(yintercept = 0, alpha = 0.25) +
  ylab("NAO Index (Annual)") +
  ylim(-2,2) +
  theme_bw()

ind_quarter <- nao_cpc %>% 
  ggplot()+
  geom_line(aes(x = Time, y = Value)) +
  geom_hline(yintercept = 0, alpha = 0.25) +
  ylab("NAO Index (Quarterly)") +
  theme_bw()

ind_var <- nao_annual %>% 
  ggplot()+
  geom_line(aes(x = Time, y = Variance)) +
  ylab("NAO Index Variance (Annual)") +
  theme_bw()

cowplot::plot_grid(ind_quarter,ind_annual, ind_var, ncol = 1, align = "hv")
```



## Synthesis  

### Correlation matrices {.tabset}

```{r corrmat_functions, echo = F, warning=FALSE, message=FALSE}
#load all clean data
invisible(lapply(file.path(clean.dir,as.list(list.files(clean.dir))),load,environment()))


get_dat <- function(field){
  #Split out data
  time <- env[env$Var == field,]$Time
  end <- max(time)
  time <- time[1:which(time == end)]
  Value <- env[env$Var == field,]$Value
  EPU <- unique(env[env$Var == field,]$EPU)
  
  if (all(is.na(as.numeric(Value))) | sd(Value, na.rm = T) == 0){
    Value <- NA #Assign as NA if not including
  } else {
    
    Value <- Value[1:length(time)]
    Value <- (Value-mean(Value, na.rm = TRUE))/sd(Value, na.rm = TRUE) #normalize
    
    #interpolate missing values
    if (any(is.na(Value))){
      Value <- approx(time, Value, n = length(Value))$y
    }
    
    #test for stationarity with Augmented Dickey Fuller
    adf <- suppressWarnings(adf.test(Value)$p.value)
    
    if (adf > 0.05){ #if non-stationary take first difference and use residuals
      mod <- arima(Value, order = c(0,1,0))
      Value <- resid(mod)
      
      adf2 <- suppressWarnings(adf.test(Value)$p.value) #check again for stationarity
      if (adf2 > 0.05){
        Value <- NA #if still non-stationary, make NA for exclusion

      }
      
    }
    
  }
  out <- data.frame(var = field,
                    value = as.numeric(Value),
                    time = time,
                    EPU = EPU)
  return(out)
}

#Bind data
env <- rbind(NE_LME_nutrients,
              ocean_sal_insitu,
              ocean_temp_insitu,
              stratification,
              wind_clean,
             slopewater,
             nao)
env$Var <- paste(env$Var, env$EPU) #create unique Var column
fields <- as.list(unique(env$Var)) #get fields for normalization
max_len <- 30
```

#### MAB Corr. Matrix 

```{r corrplot_MAB, fig.height= 8, fig.width = 10,echo = FALSE, warning=FALSE, message=FALSE}
norm_env <- lapply(fields, get_dat) #apply normalization and differencing function
norm_env <- do.call(rbind,norm_env) #convert list of dfs to single df

env_wide <- norm_env %>% #Clean up output
  dplyr::rename(Time = time, Var = var, Value = value) %>% 
  group_by(Var) %>% 
  mutate(count = n()) %>% 
  filter(count > max_len,
         !str_detect(Var,"reference"),
         EPU %in% c("MAB","all"),
         !is.na(Value)) %>% #select only data from MAB that has over 30 data points (interpolated or otherwise)
  dplyr::select(-count, -EPU) %>%
  spread(., Var, Value) %>% 
  dplyr::select(-Time)

#Fix variable names for corrmap labeling
names(env_wide) <- str_replace(names(env_wide), "MAB", "")

#Make sure only complete cases are considered
env_wide <- env_wide[complete.cases(env_wide),]

#Find correlation matrix and order by magnitude of first principal component
env_cor <- cor(env_wide)
env_order <- corrMatOrder(env_cor, order = "FPC")
env_out <- env_cor[env_order, env_order] #reorder

#heatmap
heatmaply(env_out,
        main = "MAB Env/LTL Correlation Matrix",
        margins = c(10,10,40,10),
        scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(trans = "reverse"),
        dendrogram = 'none',
        hide_colorbar = TRUE)

```

#### GB Corr. Matrix 

```{r corrplot_GB, fig.height= 8, fig.width = 10,echo = FALSE, warning=FALSE, message=FALSE}
norm_env <- lapply(fields, get_dat) #apply normalization and differencing function
norm_env <- do.call(rbind,norm_env) #convert list of dfs to single df

env_wide <-  norm_env %>% #Clean up output
  dplyr::rename(Time = time, Var = var, Value = value) %>% 
  group_by(Var) %>% 
  mutate(count = n()) %>% 
  filter(count > max_len,
         !str_detect(Var,"reference"),
         EPU %in% c("GB","all"),
         !is.na(Value)) %>% #select only data from GB that has over 30 data points (interpolated or otherwise)
  dplyr::select(-count, -EPU) %>%
  spread(., Var, Value) %>% 
  dplyr::select(-Time)

#Fix variable names for corrmap labeling
names(env_wide) <- str_replace(names(env_wide), "GB", "")

#Make sure only complete cases are considered
env_wide <- env_wide[complete.cases(env_wide),]

#Find correlation matrix and order by magnitude of first principal component
env_cor <- cor(env_wide)
env_order <- corrMatOrder(env_cor, order = "FPC")
env_out <- env_cor[env_order, env_order] #reorder

#heatmap
heatmaply(env_out,
        main = "GB Env/LTL Correlation Matrix",
        margins = c(10,10,40,10),
        scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(trans = "reverse"),
        dendrogram = 'none',
        hide_colorbar = TRUE)

```

#### GOM Corr. Matrix 

```{r corrplot_GOM, fig.height= 8, fig.width = 10,echo = FALSE, warning=FALSE, message=FALSE}
norm_env <- lapply(fields, get_dat) #apply normalization and differencing function
norm_env <- do.call(rbind,norm_env) #convert list of dfs to single df

env_wide <-  norm_env %>% #Clean up output
  dplyr::rename(Time = time, Var = var, Value = value) %>% 
  group_by(Var) %>% 
  mutate(count = n()) %>% 
  filter(count > max_len,
         !str_detect(Var,"reference"),
         EPU %in% c("GOM","all"),
         !is.na(Value)) %>% #select only data from GOM that has over 30 data points (interpolated or otherwise)
  dplyr::select(-count, -EPU) %>%
  spread(., Var, Value) %>% 
  dplyr::select(-Time)

#Fix variable names for corrmap labeling
names(env_wide) <- str_replace(names(env_wide), "GOM", "")

#Make sure only complete cases are considered
env_wide <- env_wide[complete.cases(env_wide),]

#Find correlation matrix and order by magnitude of first principal component
env_cor <- cor(env_wide)
env_order <- corrMatOrder(env_cor, order = "FPC")
env_out <- env_cor[env_order, env_order] #reorder

#heatmap
heatmaply(env_out,
        main = "GOM Env/LTL Correlation Matrix",
        margins = c(10,10,40,10),
        scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(trans = "reverse"),
        dendrogram = 'none',
        hide_colorbar = TRUE)

```


#### Methods

To create correlation matrices, we first developed a function (shown below) to normalize and assess each time series for stationarity using the Augmented Dickey Fuller (ADF) test. If a time series were found to be non-stationary, we applied a first-order differencing step before assessing for stationarity again. If upon the second ADF test the series was still shown to be non-stationarity, it was not included in further correlation analyses rather than applying higher order differencing. This step was taken to prevent over-differencing of series. To address bias introduced by small samples sizes, only time series with N > 30 were considered for correlation analyses. Group structure in correlation matrices was defined by the magnitude of the scaled and centered first principal component. 

```{r corrmat_methods, eval = FALSE, echo = TRUE}

#Normalization and differencing function

get_dat <- function(field){
  #Split out data
  time <- env[env$Var == field,]$Time
  end <- max(time)
  time <- time[1:which(time == end)]
  Value <- env[env$Var == field,]$Value
  
  if (all(is.na(as.numeric(Value))) | sd(Value, na.rm = T) == 0){
    Value <- NA #Assign as NA if not including
  } else {
    
    Value <- Value[1:length(time)]
    Value <- (Value-mean(Value, na.rm = TRUE))/sd(Value, na.rm = TRUE) #normalize
    
    #interpolate missing values
    if (any(is.na(Value))){
      Value <- approx(time, Value, n = length(Value))$y
    }
    
    #test for stationarity with Augmented Dickey Fuller
    adf <- suppressWarnings(adf.test(Value)$p.value)
    
    if (adf > 0.05){ #if non-stationary take first difference and use residuals
      mod <- arima(Value, order = c(0,1,0))
      Value <- resid(mod)
      
      adf2 <- suppressWarnings(adf.test(Value)$p.value) #check again for stationarity
      if (adf2 > 0.05){
        Value <- NA #if still non-stationary, make NA for exclusion
      }
      
    }
    
  }
  out <- data.frame(var = field,
                    value = as.numeric(Value),
                    time = time)
  return(out)
}
```

